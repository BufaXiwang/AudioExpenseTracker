//
//  VoiceRecognitionService.swift
//  AudioExpenseTracker
//
//  Created by Zhigang Yang on 13/06/2025.
//

import Foundation
import Speech
import AVFoundation
import Combine

#if targetEnvironment(simulator)
import UIKit
#endif

@MainActor
class VoiceRecognitionService: NSObject, ObservableObject {
    @Published var recordingState: RecordingState = .idle
    @Published var recognizedText: String = ""
    @Published var currentRecording: VoiceRecording?
    @Published var audioLevel: Float = 0.0
    
    private var audioEngine = AVAudioEngine()
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioSession = AVAudioSession.sharedInstance()
    private var recordingStartTime: Date?
    
    // æ·»åŠ çŠ¶æ€è·Ÿè¸ª
    private var isEngineRunning = false
    private var hasTapInstalled = false
    
    // æ¨¡æ‹Ÿå™¨æµ‹è¯•æ•°æ®
    #if targetEnvironment(simulator)
    private let simulatorTestTexts = [
        "æˆ‘ä»Šå¤©èŠ±äº†25å…ƒä¹°åˆé¤",
        "æ‰“è½¦è´¹ç”¨30å—é’±",
        "ä¹°äº†ä¸€æ¯å’–å•¡15å…ƒ",
        "è¶…å¸‚è´­ç‰©èŠ±äº†120å…ƒ",
        "åœ°é“è´¹2å…ƒ",
        "çœ‹ç”µå½±ç¥¨ä»·45å…ƒ",
        "ä¹°ä¹¦èŠ±äº†80å…ƒ",
        "æ™šé¤èšä¼š200å…ƒ"
    ]
    #endif
    
    override init() {
        super.init()
        setupSpeechRecognizer()
    }
    
    deinit {
        // ç¡®ä¿èµ„æºè¢«æ­£ç¡®æ¸…ç†
        Task { @MainActor in
            cleanupResources()
        }
    }
    
    private func setupSpeechRecognizer() {
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
        speechRecognizer?.delegate = self
    }
    
    // MARK: - èµ„æºæ¸…ç†ä¼˜åŒ–
    
    private func cleanupResourcesGracefully() {
        // ä½¿ç”¨çŠ¶æ€è·Ÿè¸ªç¡®ä¿å®‰å…¨æ¸…ç†
        if isEngineRunning && !audioEngine.inputNode.isVoiceProcessingBypassed {
            audioEngine.stop()
            isEngineRunning = false
        }
        
        if hasTapInstalled {
            audioEngine.inputNode.removeTap(onBus: 0)
            hasTapInstalled = false
        }
        
        // ä¼˜é›…å®Œæˆè¯†åˆ«è¯·æ±‚ï¼Œä¸å¼ºåˆ¶å–æ¶ˆ
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        // è®©ä»»åŠ¡è‡ªç„¶å®Œæˆï¼Œåªé‡ç½®å¼•ç”¨
        recognitionTask = nil
        
        resetAudioSession()
    }
    
    private func cleanupResources() {
        cleanupAudioEngine()
        cleanupSpeechRecognition()
        resetAudioSession()
    }
    
    private func cleanupAudioEngine() {
        if isEngineRunning {
            audioEngine.stop()
            isEngineRunning = false
        }
        
        if hasTapInstalled {
            audioEngine.inputNode.removeTap(onBus: 0)
            hasTapInstalled = false
        }
    }
    
    private func cleanupSpeechRecognition() {
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        recognitionTask?.cancel()
        recognitionTask = nil
    }
    
    private func resetAudioSession() {
        do {
            try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
        } catch {
            print("éŸ³é¢‘ä¼šè¯é‡ç½®å¤±è´¥: \(error)")
        }
    }
    
    // MARK: - æƒé™ç®¡ç†
    func requestPermissions() async -> Bool {
        let speechPermission = await requestSpeechRecognitionPermission()
        let microphonePermission = await requestMicrophonePermission()
        return speechPermission && microphonePermission
    }
    
    private func requestSpeechRecognitionPermission() async -> Bool {
        return await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status == .authorized)
            }
        }
    }
    
    private func requestMicrophonePermission() async -> Bool {
        return await withCheckedContinuation { continuation in
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                continuation.resume(returning: granted)
            }
        }
    }
    
    // MARK: - å½•åˆ¶æ§åˆ¶
    func startRecording() async throws {
        // å¦‚æœæ­£åœ¨å½•åˆ¶ï¼Œå…ˆåœæ­¢
        if isRecording {
            stopRecording()
            // ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿èµ„æºè¢«æ¸…ç†
            try await Task.sleep(nanoseconds: 500_000_000) // 0.5ç§’
        }
        
        guard await requestPermissions() else {
            throw VoiceRecognitionError.permissionDenied
        }
        
        guard speechRecognizer?.isAvailable == true else {
            throw VoiceRecognitionError.speechRecognitionUnavailable
        }
        
        // ç¡®ä¿ä¹‹å‰çš„èµ„æºè¢«æ¸…ç†
        cleanupResources()
        
        do {
            try await configureAudioSession()
            try await startSpeechRecognition()
            
            recordingState = .recording
            recordingStartTime = Date()
            recognizedText = ""
        } catch {
            // å¦‚æœå¯åŠ¨å¤±è´¥ï¼Œæ¸…ç†èµ„æº
            cleanupResources()
            throw error
        }
    }
    
    func stopRecording() {
        guard isRecording else { return }
        
        print("ğŸ¤ [VOICE DEBUG] åœæ­¢å½•éŸ³ï¼Œå½“å‰è¯†åˆ«æ–‡æœ¬: '\(recognizedText)'")
        
        recordingState = .processing
        
        // å…ˆä¼˜é›…åœ°ç»“æŸè¯­éŸ³è¯†åˆ«è¯·æ±‚
        recognitionRequest?.endAudio()
        
        // ç»™è¯­éŸ³è¯†åˆ«ä¸€ç‚¹æ—¶é—´å®Œæˆå¤„ç†
        Task {
            // ç­‰å¾…çŸ­æš‚æ—¶é—´è®©è¯­éŸ³è¯†åˆ«å®Œæˆ
            try? await Task.sleep(nanoseconds: 200_000_000) // 0.2ç§’
            
            await MainActor.run {
                // ä½¿ç”¨ä¼˜é›…çš„æ¸…ç†æ–¹å¼
                cleanupResourcesGracefully()
                
                // åˆ›å»ºå½•åˆ¶è®°å½•
                if let startTime = recordingStartTime {
                    let duration = Date().timeIntervalSince(startTime)
                    currentRecording = VoiceRecording(
                        transcribedText: recognizedText,
                        duration: duration,
                        recordingDate: startTime,
                        isProcessing: false
                    )
                    
                    print("ğŸ¤ [VOICE DEBUG] åˆ›å»ºå½•éŸ³è®°å½•:")
                    print("ğŸ¤ [VOICE DEBUG] - è½¬å½•æ–‡æœ¬: '\(recognizedText)'")
                    print("ğŸ¤ [VOICE DEBUG] - å½•éŸ³æ—¶é•¿: \(duration)ç§’")
                    print("ğŸ¤ [VOICE DEBUG] - æ–‡æœ¬é•¿åº¦: \(recognizedText.count)å­—ç¬¦")
                }
                
                recordingState = .completed
                print("ğŸ¤ [VOICE DEBUG] å½•éŸ³çŠ¶æ€è®¾ç½®ä¸ºå®Œæˆ")
            }
        }
    }
    
    private func configureAudioSession() async throws {
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: [.duckOthers, .allowBluetooth])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            throw VoiceRecognitionError.audioEngineError
        }
    }
    
    private func startSpeechRecognition() async throws {
        // ç¡®ä¿ä¹‹å‰çš„ä»»åŠ¡è¢«æ¸…ç†
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // åˆ›å»ºæ–°çš„è¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            throw VoiceRecognitionError.failedToCreateRequest
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        // å¦‚æœå¯ç”¨ï¼Œä½¿ç”¨è®¾å¤‡ä¸Šçš„è¯†åˆ«
        if #available(iOS 13.0, *) {
            recognitionRequest.requiresOnDeviceRecognition = false
        }
        
        let inputNode = audioEngine.inputNode
        
        // åˆ›å»ºè¯†åˆ«ä»»åŠ¡
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            Task { @MainActor in
                guard let self = self else { return }
                
                if let result = result {
                    self.recognizedText = result.bestTranscription.formattedString
                    print("ğŸ¤ [VOICE DEBUG] è¯†åˆ«åˆ°æ–‡æœ¬: '\(self.recognizedText)'")
                    print("ğŸ¤ [VOICE DEBUG] æ˜¯å¦æœ€ç»ˆç»“æœ: \(result.isFinal)")
                }
                
                if let error = error {
                    print("ğŸ¤ [VOICE DEBUG] è¯†åˆ«é”™è¯¯: \(error)")
                    // å¿½ç•¥ç‰¹å®šçš„ç³»ç»Ÿé”™è¯¯
                    let nsError = error as NSError
                    
                    // å¿½ç•¥æ­£å¸¸åœæ­¢å½•éŸ³æ—¶çš„å–æ¶ˆé”™è¯¯
                    if nsError.domain == "kLSRErrorDomain" && nsError.code == 301 {
                        // 301: "Recognition request was canceled" - æ‰‹åŠ¨åœæ­¢å½•éŸ³æ—¶çš„æ­£å¸¸ç°è±¡
                        print("å¿½ç•¥å½•éŸ³åœæ­¢é”™è¯¯ (Code: \(nsError.code)): \(error.localizedDescription)")
                        return
                    }
                    
                    if nsError.domain == "kAFAssistantErrorDomain" && (nsError.code == 1101 || nsError.code == 1110) {
                        // 1101: ç³»ç»Ÿå†…éƒ¨é”™è¯¯ï¼Œå¯ä»¥å¿½ç•¥
                        // 1110: "No speech detected" - å½•éŸ³ç»“æŸåçš„æ­£å¸¸ç°è±¡
                        print("å¿½ç•¥ç³»ç»Ÿè¯­éŸ³è¯†åˆ«é”™è¯¯ (Code: \(nsError.code)): \(error.localizedDescription)")
                        return
                    }
                    
                    self.handleError(error)
                }
            }
        }
        
        // å®‰è£…éŸ³é¢‘tap
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // ç¡®ä¿æ²¡æœ‰å·²å®‰è£…çš„tap
        if hasTapInstalled {
            inputNode.removeTap(onBus: 0)
            hasTapInstalled = false
        }
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            recognitionRequest.append(buffer)
            
            // è®¡ç®—éŸ³é¢‘ç”µå¹³
            self?.calculateAudioLevel(from: buffer)
        }
        hasTapInstalled = true
        
        // å¯åŠ¨éŸ³é¢‘å¼•æ“
        audioEngine.prepare()
        try audioEngine.start()
        isEngineRunning = true
    }
    
    nonisolated private func calculateAudioLevel(from buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData?[0] else { return }
        
        let channelDataValueArray = stride(from: 0, to: Int(buffer.frameLength), by: buffer.stride).map { channelData[$0] }
        let rms = sqrt(channelDataValueArray.map { $0 * $0 }.reduce(0, +) / Float(channelDataValueArray.count))
        
        Task { @MainActor in
            // ä½¿ç”¨å¼±å¼•ç”¨é¿å…æ½œåœ¨çš„å¾ªç¯å¼•ç”¨
            guard !Task.isCancelled else { return }
            
            let normalizedLevel = min(rms * 10, 1.0) // é™åˆ¶åœ¨0-1èŒƒå›´å†…
            
            // æ·»åŠ éŸ³é¢‘çº§åˆ«å¹³æ»‘å¤„ç†
            let smoothingFactor: Float = 0.3
            self.audioLevel = self.audioLevel * (1 - smoothingFactor) + normalizedLevel * smoothingFactor
        }
    }
    
    nonisolated private func handleError(_ error: Error) {
        print("è¯­éŸ³è¯†åˆ«é”™è¯¯: \(error)")
        
        Task { @MainActor in
            guard !Task.isCancelled else { return }
            
            // æ¸…ç†èµ„æº
            cleanupResources()
            
            // åˆ›å»ºæ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            let errorMessage = createUserFriendlyErrorMessage(error)
            recordingState = .error(errorMessage)
        }
    }
    
    @MainActor
    private func createUserFriendlyErrorMessage(_ error: Error) -> String {
        if let recognitionError = error as? VoiceRecognitionError {
            return recognitionError.localizedDescription
        }
        
        // æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        let nsError = error as NSError
        
        switch nsError.domain {
        case "kAFAssistantErrorDomain":
            switch nsError.code {
            case 1101:
                return "è¯­éŸ³è¯†åˆ«æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
            case 1110:
                return "æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡æ–°å½•åˆ¶"
            case 203:
                return "ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®"
            default:
                return "è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡æ–°å°è¯•"
            }
        case NSURLErrorDomain:
            return "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        default:
            return "å½•åˆ¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š\(error.localizedDescription)"
        }
    }
    
    // MARK: - çŠ¶æ€æŸ¥è¯¢
    var isRecording: Bool {
        recordingState.isRecording
    }
    
    var canRecord: Bool {
        recordingState == .idle || recordingState == .completed
    }
    
    // MARK: - èµ„æºç›‘æ§
    
    @MainActor
    func getResourceStatus() -> ResourceStatus {
        return ResourceStatus(
            isEngineRunning: isEngineRunning,
            hasTapInstalled: hasTapInstalled,
            hasActiveTask: recognitionTask != nil,
            hasActiveRequest: recognitionRequest != nil,
            audioSessionActive: audioSession.isOtherAudioPlaying
        )
    }
    
    @MainActor
    func performHealthCheck() -> VoiceServiceHealthStatus {
        // æ£€æŸ¥åŸºæœ¬ç»„ä»¶
        guard let recognizer = speechRecognizer else {
            return .critical("è¯­éŸ³è¯†åˆ«å™¨æœªåˆå§‹åŒ–")
        }
        
        guard recognizer.isAvailable else {
            return .degraded("è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨")
        }
        
        // æ£€æŸ¥æƒé™
        let speechAuthStatus = SFSpeechRecognizer.authorizationStatus()
        let microphoneAuthStatus = AVAudioSession.sharedInstance().recordPermission
        
        if speechAuthStatus != .authorized {
            return .critical("è¯­éŸ³è¯†åˆ«æƒé™æœªæˆæƒ")
        }
        
        if microphoneAuthStatus != .granted {
            return .critical("éº¦å…‹é£æƒé™æœªæˆæƒ")
        }
        
        // æ£€æŸ¥éŸ³é¢‘å¼•æ“çŠ¶æ€
        if isRecording && !isEngineRunning {
            return .degraded("éŸ³é¢‘å¼•æ“çŠ¶æ€å¼‚å¸¸")
        }
        
        return .healthy
    }
}

// MARK: - SFSpeechRecognizerDelegate
extension VoiceRecognitionService: SFSpeechRecognizerDelegate {
    nonisolated func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        print("è¯­éŸ³è¯†åˆ«å¯ç”¨æ€§å˜åŒ–: \(available)")
        
        Task { @MainActor in
            if !available && isRecording {
                stopRecording()
            }
        }
    }
}

// MARK: - èµ„æºçŠ¶æ€ç›‘æ§
struct ResourceStatus {
    let isEngineRunning: Bool
    let hasTapInstalled: Bool
    let hasActiveTask: Bool
    let hasActiveRequest: Bool
    let audioSessionActive: Bool
    
    var isHealthy: Bool {
        // å¦‚æœæ­£åœ¨å½•åˆ¶ï¼Œåº”è¯¥æœ‰æ´»è·ƒçš„ç»„ä»¶
        return !(isEngineRunning && !hasActiveTask)
    }
    
    var description: String {
        var components: [String] = []
        if isEngineRunning { components.append("å¼•æ“è¿è¡Œä¸­") }
        if hasTapInstalled { components.append("éŸ³é¢‘ç›‘å¬") }
        if hasActiveTask { components.append("è¯†åˆ«ä»»åŠ¡") }
        if hasActiveRequest { components.append("è¯†åˆ«è¯·æ±‚") }
        if audioSessionActive { components.append("éŸ³é¢‘ä¼šè¯") }
        
        return components.isEmpty ? "ç©ºé—²çŠ¶æ€" : components.joined(separator: ", ")
    }
}

enum VoiceServiceHealthStatus: Equatable {
    case healthy
    case degraded(String)
    case critical(String)
    
    var isOperational: Bool {
        switch self {
        case .healthy, .degraded:
            return true
        case .critical:
            return false
        }
    }
    
    var description: String {
        switch self {
        case .healthy:
            return "è¯­éŸ³æœåŠ¡æ­£å¸¸"
        case .degraded(let message):
            return "è¯­éŸ³æœåŠ¡å¼‚å¸¸: \(message)"
        case .critical(let message):
            return "è¯­éŸ³æœåŠ¡ä¸å¯ç”¨: \(message)"
        }
    }
}

// MARK: - é”™è¯¯å®šä¹‰
enum VoiceRecognitionError: LocalizedError {
    case permissionDenied
    case speechRecognitionUnavailable
    case failedToCreateRequest
    case audioEngineError
    
    var errorDescription: String? {
        switch self {
        case .permissionDenied:
            return "éœ€è¦éº¦å…‹é£å’Œè¯­éŸ³è¯†åˆ«æƒé™"
        case .speechRecognitionUnavailable:
            return "è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨"
        case .failedToCreateRequest:
            return "æ— æ³•åˆ›å»ºè¯­éŸ³è¯†åˆ«è¯·æ±‚"
        case .audioEngineError:
            return "éŸ³é¢‘å¼•æ“é…ç½®å¤±è´¥"
        }
    }
} 
